;;; Starting to mess around with elisp -*- lexical-binding: t; -*-

;; I figure I might be in emacs'land for quite a while so I want to
;; learn some elisp programming but I don't want to clutter up my
;; init.el file so I'll just put this stuff here. Part of this too
;; will probably be me just messing around and doing what I do best:
;; waxing poetic on what I think about various programming concepts.

;; TODO: I don't know how much I should be adding these "require"
;; statements. I guess just as necessary? It's also got me wondering
;; about how libraries declare that they need to rely on other
;; libraries to work such that those dependencies get downloaded too.
;; I don't know a lot about this stuff. It all feels very... global
;; which is probably convenient but also bad? Like I never explicitly
;; installed map.el or ht.el but they both exist. Were they installed
;; from another package? If so then if I remove said package then if
;; I've used that code then my code just breaks one day. I hate that
;; action at a distance stuff. Or maybe some of them were installed by
;; default as part of emacs?
(require 'benchmark)
(require 'thunk)
(require 'dom)
(require 'iso8601)

(defun array-last-item (arr)
  "Returns the last item of an array. Pretty trivial but I just
wanted it so I can be little more declaritive with my code.
Original motiviation for this function's creation was to find out
the port of a network process created by emacs which appears to
be in the last element of the array generated by the function:

(process-contact process :local)"
  (aref arr (1- (length arr))))

(defun equal? (o1 o2)
  "Return t if two objects have a similar structure and contents.
This was created because the built in \"equal\" function does not
work for hash tables but I want it to."
  (if (and (ht? o1) (ht? o2))
      (ht-equal? o1 o2)
    (equal o1 o2)))

;; TODO: Apparently a fixed point can be thought of as a "periodic
;; point" (https://en.wikipedia.org/wiki/Periodic_point) with a period
;; of 1. I wonder if making that sort generalization is useful. Heck
;; even a "periodic point" could probably be further generalized as
;; "return the latest point when a predicate applied to the orbit of
;; an iterated function returns true". I wonder if THAT generalization
;; is useful at all. Related links:
;; https://en.wikipedia.org/wiki/Iterated_function (so I guess that
;; clojure function "iterate" is well named:
;; https://clojuredocs.org/clojure.core/iterate also, for my
;; knowledge, here is some info about why clojure recommends not
;; passing a function with side effects into iterate:
;; https://www.reddit.com/r/Clojure/comments/orwvmq/iterate_does_f_really_must_be_free_of_sideeffects/.
;; There's also this:
;; https://stuartsierra.com/2015/08/25/clojure-donts-lazy-effects).
;; Just keep it in mind as a way to generalize the idea of controlling
;; iteration!
(defun fixed-point (f x &optional n)
  "Finds the fixed point of F given initial input X. Optionally
provide a max number N of iterations before the computed value is
returned. Note that one gotcha to think about is that if X is
some sort of reference that F modifies, F will need to make a
copy of X so this function can have a proper unique \"previous\"
and \"next\" value to compare.

Related links:
- https://github.com/RutledgePaulV/missing/blob/fb1b113ada2be09cf04f42022731ef317287adf6/src/missing/core.clj#L326-L341
- https://gigamonkeys.com/book/loop-for-black-belts.html"
  (if (null n)
      (cl-loop for prev = x then next
	       for next = (funcall f prev)
	       until (equal? prev next)
	       finally return prev)
    (cl-loop for prev = x then next
	     repeat n
	     for next = (funcall f prev)
	     until (equal? prev next)
	     finally return prev)))

;; TODO: Writing this multi-arity defun macro was fun. Do some other
;; macros to like maybe a defun-curried macro for example.

(defun defun-multi-arity-helper-fn (name docstring multi-arity-forms)
  (let ((defun-arg-name (gensym)))
    (cl-flet ((multi-arity-form-to-cl-case-clause
	       ;; Apparently the cl-* forms in elisp allow for
	       ;; destructring of arguments like in clojure, neat:
	       ;; https://www.gnu.org/software/emacs/manual/html_node/cl/Argument-Lists.html
	       ;; It almost makes me just want to use cl-defun all the
	       ;; time instead of defun... hmmm. Ah well! I'll keep it
	       ;; in mind though.
	       ((arglist &rest body))
	       `(,(length arglist)
		 (let ,(-map-indexed (lambda (index arglist-param)
				       (list arglist-param `(nth ,index ,defun-arg-name)))
				     arglist)
		   ,@body))))
      `(defun ,name (&rest ,defun-arg-name)
	 ,docstring
	 (cl-case (length ,defun-arg-name)
	   ,@(-map #'multi-arity-form-to-cl-case-clause multi-arity-forms)
	   (otherwise (error "multi arity function called with a number of arguments for which this function has no definition.")))))))

(defmacro defun-multi-arity (name &optional docstring &rest fn-forms)
  "Creates a function which can accept different numbers of
parameters (i.e. \"arities\") when invoked. It's basically
overloading a function based on the number of arguments. Under
the hood, it dispatches to a different code path depending on how
many arguments were passed into the multi-arity function. TODO:
It does NOT allow for having any &rest parameters or anything
like but if I was being 100% good, it probably should. The &rest
parameter would show up on the last arg and if it does then that
case would match if the number of args is >= to the required
args.

The original motivation for this (as are many other things in my
travels) was clojure:
https://clojure.org/guides/learn/functions#_multi_arity_functions
More specifically, I wanted to make a \"lazy-range\" function and
copy it's implementation from clojure's \"range\" function:
https://github.com/clojure/clojure/blob/38bafca9e76cd6625d8dce5fb6d16b87845c8b9d/src/clj/clojure/core.clj#L3019-L3039
but implementing that with a plain ol' defun plus &optional
parameters felt a bit messy because you have to manually check
how many arguments are passed and work accordingly AND, in this
case, the first parameter has a different meaning when the
function is invoked with more parameters and that made it feel
hard to name the variable properly. For example, when you
call (range 10), the first parameter \"10\" signifies the LAST
number exclusive in the generated sequence but when invoking the
function like (range 10 100), the number \"10\" now means the
FIRST number inclusive in the range. So I wanted to name the
variable something like \"max-or-min\" and that felt weird. Being
able to define my own multi-arity functions fixes these problems.

I also feel like being able to define a multi-arity function is a
potentially cleaner way to define a recursive function which ends
up having to accumulate a result in one of it's parameters. Such
functions often have a clean interface which do NOT expose the
accumulation parameter and then, inside, they define a function
which has that extra accumulation parameter and just call out to
that function."
  (unless (stringp docstring)
    (setq fn-forms (cons docstring fn-forms))
    (setq docstring nil))
  (defun-multi-arity-helper-fn name docstring fn-forms))

(defun bad-defun-curried-helper-fn (name arglist docstring body)
  `(defun ,name (,(car arglist)) ,docstring
	  ,@(-reduce-r-from (lambda (arg acc)
			     (list (append `(lambda (,arg)) acc)))
			    body
			    (cdr arglist))))

(defmacro bad-defun-curried (name arglist &optional docstring &rest body)
  "Used to define a function that is curried. Just made this for
fun, in practice I don't think there's much use for it since
invoking the curried function is so atrocious syntactically. For
example:

Given this function:

(defun-curried lucas-test (x y z)
  (+ x y z))

To finally get a value, we must do:

(funcall (funcall (lucas-test 1) 2) 3)

*shudder*. I get the feeling there is a better way to do define
this curried macro. Like maybe if a function takes 4 arguments we
return a lambda with 4 optional arguments and if 0 get passed we
return the same lambda, if 1 gets passed then we return a lambda
of 3 arguments, if 2 get passed we return a lambda of 2
arguments, etc... Then you could pass in as many values as you'd
like without having to have all these funcall invocations."
  (unless (stringp docstring)
    (setq body (cons docstring body))
    (setq docstring nil))
  (bad-defun-curried-helper-fn name arglist docstring body))

(bad-defun-curried lucas-test (x y z)
  (+ x y z))

(funcall (funcall (lucas-test 1) 2) 3)

;; TODO: Returning buffers with data feels kind of weird but also it
;; seems like a pattern with emacs so I guess it's okay. One thing I
;; want to research though is what emacs does with all these random
;; buffers. Do they get cleaned up faster than buffers that get edited
;; normally?

(defun url-retrieve-body-synchronously (url)
  "Returns a buffer filled with just the message body of the http
response from URL"
  (let ((buffer (url-retrieve-synchronously url)))
    (save-excursion
      (set-buffer buffer)
      (goto-char (point-min))
      ;; There is an empty line between the header stuff and the
      ;; message body
      (re-search-forward "^$")
      (forward-char)
      (delete-region (point-min) (point))
      buffer)))

(defun url-retrieve-body-synchronously-str (url)
  "A second attempt at writing the function to get just the http
response by using less buffer related logic. I'm so early on in
emacs programming that I don't really know yet if working with
data in buffers is a common way to do things but on first pass it
feels weird so I wanted to see what the code looks like if I just
do normal string manipulation stuff. Unfortunately the base
function which gets the data from a url does store it in a buffer
but it's easy to get that data as a string. Also, I learned that
the function url-http is what ultimately generates the temporary
buffer. Took me a bit to find that. That function is stored in
some sort of property list? Not really sure how it all works
yet."
  (let ((resp (with-current-buffer
		  (url-retrieve-synchronously url)
		(buffer-string))))
    (substring resp (1+ (string-match "^$" resp)))))

(defmacro comment (&rest body)
  "Evaluates the body and yields nil. Borrowed from clojure:
https://clojuredocs.org/clojure.core/comment Useful for adding
code to a file which is illustrative but you don't want
executed." nil)

(defun parse-html-from-url (url)
  "Parses html from the data from a URL. I created this because
it seems that the functions which download data from a URL create
a temporary buffer to store that data and I wanted to make sure
that the temporary buffer gets deleted (I assume this is a good
thing to do?)."
  (let* ((buffer (url-retrieve-body-synchronously url))
	 (dom
	  (with-current-buffer
	      buffer
	    ;; TODO: I cannot for the life of me get the "base url"
	    ;; parameter to libxml-parse-html-region working. I
	    ;; assumed that it would slap on the base url to any href
	    ;; attributes which were absolute or relative paths but it
	    ;; doesn't seem to work that way, It doesn't seem to do
	    ;; anything frankly.
	    (libxml-parse-html-region (point-min) (point-max) url))))
    (kill-buffer buffer)
    dom))

(defun url-normalize-url-for-sitecrawl (urlobj)
  "Normalizes a url roughly following the description here:
https://en.wikipedia.org/wiki/URI_normalization. The use case for
me wanting to compare urls and tell if they are the same for the
site crawling algorithm."
  (setf (url-filename urlobj) (car (url-path-and-query urlobj)))
  (setf (url-target urlobj) nil)
  ;; Remove "." and ".." segments
  (let ((path (-remove (lambda (s) (equal s ".")) (split-string (url-filename urlobj) "/")))
	acc)
    (while path
      (if (equal (car path) "..")
	  (setf acc (cdr acc))
	(setf acc (cons (car path) acc)))
      (setf path (cdr path)))
    (setf (url-filename urlobj) (combine-and-quote-strings (nreverse acc) "/")))
  (when (equal "" (url-filename urlobj))
    (setf (url-filename urlobj) "/"))
  urlobj)

(defun url-relative-to-absolute (urlobj base-urlobj)
  "Converts a relative url to an absolute one or just returns the
url if it is already absolute. Use case is if we see a url inside
an href on a html page."
  (if (and (null (url-type urlobj))
	   (null (url-host urlobj)))
      (let ((res (copy-sequence base-urlobj)))
	(setf (url-filename res)
	      (cond
	       ((string-match "^/" (url-filename urlobj))
		(url-filename urlobj))
	       ((zerop (length (url-filename base-urlobj)))
		(concat "/" (url-filename urlobj)))
	       ;; When the href is relative, you go up to the level of
	       ;; the directory the current page is on and then go
	       ;; from there.
	       (t (concat (combine-and-quote-strings (-drop-last 1 (split-string (url-filename base-urlobj) "/")) "/")
			  "/"
			  (url-filename urlobj)))))
	res)
    urlobj))

(defun parse-buffer-as-html-and-get-urls-within-same-site (buffer base-url)
  "Parses BUFFER as html and then pulls out all urls from that
html that are on the same site as BASE-URL. This logic was pulled
out because it is common to both the non-concurrent and
concurrent implementations of the site crawling algorithm. I feel
like this is a great example of how functions are so much more
useful if they're just given what they need instead of going out
to get them. I'm curious if this function should even bother with
knowledge of the buffer, wouldn't it be cleaner if it just had
the html structure? Eh, maybe it's fine though.

Also debated not trying common-alize the logic as per:
https://sandimetz.com/blog/2016/1/20/the-wrong-abstraction but
figured that this logic was literally the same, might as well do
it."
  (let* ((dom
	  (with-current-buffer
	      buffer
	    ;; TODO: I cannot for the life of me get the "base url"
	    ;; parameter to libxml-parse-html-region working. I
	    ;; assumed that it would slap on the base url to any href
	    ;; attributes which were absolute or relative paths but it
	    ;; doesn't seem to work that way, It doesn't seem to do
	    ;; anything frankly.
	    (libxml-parse-html-region (point-min) (point-max))))
	 (anchor-tags (dom-by-tag dom 'a))
	 (base-urlobj (url-generic-parse-url base-url)))
    (kill-buffer buffer)
    (->> anchor-tags
	 (-map (lambda (anchor-tag) (dom-attr anchor-tag 'href)))
	 ;; Sometimes an anchor tag has no href attribute so we remove
	 ;; those before moving on.
	 (-remove #'null)
	 (-map #'url-generic-parse-url)
	 (-map (lambda (hrefobj) (url-relative-to-absolute hrefobj base-urlobj)))
	 (-filter (lambda (absolute-hrefobj)
		    (-contains? '("http" "https") (url-type absolute-hrefobj))))
	 (-map #'url-normalize-url-for-sitecrawl)
	 (-filter (lambda (absolute-hrefobj)
		    (equal (url-host base-urlobj)
			   (url-host absolute-hrefobj))))
	 (-map #'url-recreate-url))))

;; do we want a function which takes the base url and the already
;; parsed html? That could be the common behavior? Or the common
;; behavior could be a buffer which needs to be parsed as html? This
;; logic will kill the buffer after I guess? Yeah, because then this
;; function as is could be passed as the callback to url-retrieve and
;; it can be used in this function as well. We could even just have
;; the function implicitly work in the context of the correct buffer
;; or should we explicitly pass in the buffer? Probably explicitly
;; pass it.

(defun get-urls-within-same-site (base-url)
  "Finds all urls within BASE-URL which have the same host as
BASE-URL."
  (parse-buffer-as-html-and-get-urls-within-same-site (url-retrieve-synchronously base-url) base-url))

(defun ht-collect-keys (pred table)
  "Builds a list of all keys from TABLE that satisfy PRED,
which is a function of two arguments: KEY and VALUE."
  (let (results)
    (ht-each (lambda (key val)
	       (when (funcall pred key val)
		 (setq results (cons key results))))
	     table)
    results))

(defun crawl-site-one-level (site-hash-table)
  "Grows the hash table by crawling all urls who's value
indicates it needs to be crawled and, for any newly seen urls,
add them to the hash table so they will be crawled on future
invocations of this function."
  ;; TODO: Could I structure this function to better seperate the side
  ;; effect and non side effect portions? Or isolate the side effect
  ;; stuff to a smaller area? Because right now the
  ;; get-urls-within-same-site function performs IO. I guess I like
  ;; the idea of being able to test this in a functional manner.
  (let* ((urls-to-crawl
	  (ht-collect-keys (lambda (_ value) (eq value :crawl-me)) site-hash-table))
	 (discovered-urls
	  (-map (-compose #'-distinct #'get-urls-within-same-site) urls-to-crawl))
	 ;; Doing stuff like this (i.e. having to make copies of data
	 ;; structures for algorithms to work (in this case it's
	 ;; because the fixed-point algorithm must be able to compare
	 ;; a separate "previous" and "next" value)) is very annoying
	 ;; and it makes me want functional programming.
	 (site-hash-table (ht-copy site-hash-table)))
    ;; Updates the url keys's values to be the urls that were found on
    ;; those pages.
    (cl-mapc (lambda (url discovered-urls)
	       (ht-set! site-hash-table url discovered-urls))
	     urls-to-crawl
	     discovered-urls)
    ;; Adds the newly discovered urls to the hash table as urls to be
    ;; crawled.
    (cl-mapc (lambda (newly-discovered-url)
	       (unless (ht-contains? site-hash-table newly-discovered-url)
		 (ht-set! site-hash-table newly-discovered-url :crawl-me)))
	     (-mapcat #'identity discovered-urls))
    site-hash-table))

(defun crawl-site (url-or-hash-table &optional depth-limit)
  "Crawls a site starting from a given url. Being able to provide
a single url is sort of a convenience, what the website crawling
function truly operates on is a hash table and that may be passed
in as well which can be handy if you, say, start crawling a
website, save the results so far, and continue crawling at a
later time."
  (if (stringp url-or-hash-table)
      (let ((normalized-url (url-recreate-url (url-normalize-url-for-sitecrawl (url-generic-parse-url url-or-hash-table)))))
	;; TODO: I'm not sure if I like this :crawl-me keyword getting
	;; repeated everywhere, how do we fix that? A let binding over
	;; these two functions?
	(fixed-point #'crawl-site-one-level (ht (normalized-url :crawl-me)) depth-limit))
    (fixed-point #'crawl-site-one-level url-or-hash-table depth-limit)))

(defun crawl-site-just-one-url (site-hash-table)
  "The single unit of work for crawling a website but it just
crawls a single url instead of every one currently in the hash
table."
  (-if-let (url-to-crawl (car (ht-find (lambda (_ value)
					 (equal :crawl-me value))
				       site-hash-table)))
      (let ((discovered-urls (-distinct (get-urls-within-same-site url-to-crawl)))
	    (site-hash-table (ht-copy site-hash-table)))
	(ht-set! site-hash-table url-to-crawl discovered-urls)
	(-each discovered-urls
	  (lambda (url)
	    (unless (ht-contains? site-hash-table url)
	      (ht-set! site-hash-table url :crawl-me))))
	site-hash-table)
    site-hash-table))

(defun crawl-site2 (url-or-hash-table &optional num-pages-to-crawl)
  "Crawls a site starting from a given url. A second
implementation where the function which does 1 unit of work
crawls just one site."
  (if (stringp url-or-hash-table)
      (let ((normalized-url (url-recreate-url (url-normalize-url-for-sitecrawl (url-generic-parse-url url-or-hash-table)))))
	(fixed-point #'crawl-site-just-one-url (ht (normalized-url :crawl-me))))
    (fixed-point #'crawl-site-just-one-url url-or-hash-table num-pages-to-crawl)))

(defun crawl-site3 (url-or-site-hash-table &optional depth-limit)
  "Crawls a site given a hash table describing the sites to
crawl. Just continuing to implement this function in different
ways!"
  (let ((site-hash-table (if (stringp url-or-site-hash-table)
			     (ht ((url-recreate-url (url-normalize-url-for-sitecrawl (url-generic-parse-url url-or-site-hash-table))) :crawl-me))
			   url-or-site-hash-table)))
    (while (and (if depth-limit (> depth-limit 0) t)
		(ht-find (lambda (_ value) (eq value :crawl-me)) site-hash-table))
      (-each (ht-collect-keys (lambda (_ value) (eq value :crawl-me)) site-hash-table)
	(lambda (url-to-crawl)
	  (let ((discovered-urls (-distinct (get-urls-within-same-site url-to-crawl))))
	    (ht-set! site-hash-table url-to-crawl discovered-urls)
	    (-each discovered-urls
	      (lambda (discovered-url)
		(unless (ht-contains? site-hash-table discovered-url)
		  (ht-set! site-hash-table discovered-url :crawl-me)))))))
      (setq depth-limit (if depth-limit (1- depth-limit) depth-limit)))
    site-hash-table))

;; TODO: In messing around with url-retrieve, I noticed that it failed
;; to work for https://www.braveclojure.com/. Turned out the issue
;; (which I discovered by doing a plain ol' curl in a terminal) was
;; that the certs on my mac were outdated and so it thought the cert
;; on that website was not valid. Anyway, url-retrieve-synchronously
;; worked somehow but url-retrieve would just hang and the process
;; would be stuck in the "connect" phase so I assumed something was
;; wrong with it but it turned out to be that. I wish I had more
;; introspection on that going wrong. Anyway I'm not really sure what
;; this todo is for. Maybe to better understand networking stuff in
;; emacs?

(defun crawl-site-one-level-parallel (site-hash-table)
  "Crawls one level of websites but tries to do it concurrently.
This is my first attempt to do some concurrent stuff in emacs and
it feels jaaaaanky. Maybe I just haven't found the right
primitives or don't know how to properly piece things together or
maybe I just don't fully understand what I have available but the
primitives I found feel super... leaky. Like if you're not
careful they won't work properly. Sometimes this implementation
works more quickly than the others and other times it seems to
take much much longer lol. Actually, on closer inspection, I
think this implementation just straight up doesn't work 100%
correctly so that's a shame! I say that because I crawled
https://en.wikipedia.org/wiki/URI_normalization to a depth of 2
and this version took waaaaaayyyy longer AND the hash table
returned was not the same as a different implementation which I
know works. I think perhaps the issue was that it spawned too
many processes though and errors started happening. I should make
a bounded implementation."
  (let* ((urls-to-crawl (ht-collect-keys (lambda (key val) (eq :crawl-me val)) site-hash-table))
	 (discovered-urls (-repeat (length urls-to-crawl) :not-discovered))
	 (async-buffers nil)
	 (site-hash-table (ht-copy site-hash-table)))
    (-each-indexed urls-to-crawl
      (lambda (url-to-crawl-index url-to-crawl)
	(setq async-buffers (cons (url-retrieve url-to-crawl
						(lambda (_ index)
						  (setf (nth index discovered-urls)
							(-distinct (parse-buffer-as-html-and-get-urls-within-same-site (current-buffer) url-to-crawl))))
						(list url-to-crawl-index))
				  async-buffers))))
    ;; Wait for all the callbacks to happen. This feels hacky as
    ;; shiiiiiit. I wonder if emacs has better primitives for doing
    ;; stuff like this or if this is just how it's gotta be.
    ;; Originally, I was doing just one pass through the list that
    ;; gets populated with discovered urls because I was assuming that
    ;; even if a slot isn't filled in, once "accept-process-output"
    ;; returns then that slot will be filled in. That does NOT appear
    ;; to be the case. It makes sense I suppose, the "process" will
    ;; get back data sure, but that doesn't mean the callback has been
    ;; called yet. Adding the loop to keep checking everything seems
    ;; to get us where we need to be. I still don't feel super
    ;; confident in it tbh but this feels good enough for these little
    ;; playing around adventures of mine. EDIT: I've gained a little
    ;; better knowledge of emacs's processing model and I think doing
    ;; what I did here is correct and necessary. Calling
    ;; accept-process-output is necessary for emacs to be able to
    ;; consume the output from the network process which is getting
    ;; the data from the website:
    ;; https://www.gnu.org/software/emacs/manual/html_node/elisp/Output-from-Processes.html
    ;; accept-process-output basically yields control to the process
    ;; again. As we can see in that link, other functions like sit-for
    ;; and sleep-for would also yield control in a similar manner. If
    ;; we didn't use one of those functions then we'd have an infinite
    ;; loop where we'd never get any new data on the wire and never
    ;; process it. Perhaps we could simplify the condition a bit by
    ;; defining a counter to be the number of urls to crawl and having
    ;; each callback decrement said counter (which is safe to do
    ;; because, again, elisp is single threaded) but I'll probably
    ;; just leave this be for now. Heck! I could also do the updating
    ;; of the hash table in the callbacks because, again, it's all
    ;; single threaded! This implementation is kind of busted anyway
    ;; though because it can spawn an infinite number of processes and
    ;; that seems to give emacs some difficulties so I'll just leave
    ;; it here for historical fun.
    (let ((every-url-retrieval-not-done t))
      (while every-url-retrieval-not-done
	(setq every-url-retrieval-not-done nil)
	(cl-mapc (lambda (proc discovered-urls)
		   (when (eq discovered-urls :not-discovered)
		     (setq every-url-retrieval-not-done t)
		     (accept-process-output proc 1)))
		 (-map #'get-buffer-process async-buffers)
		 discovered-urls)))
    (cl-mapc (lambda (url-to-crawl discovered-urls)
	       (ht-set! site-hash-table url-to-crawl discovered-urls))
	     urls-to-crawl
	     discovered-urls)
    (cl-mapc (lambda (newly-discovered-url)
	       (unless (ht-contains? site-hash-table newly-discovered-url)
		 (ht-set! site-hash-table newly-discovered-url :crawl-me)))
	     (-flatten-n 1 discovered-urls))
    site-hash-table))

(defun crawl-site4 (url-or-hash-table &optional depth-limit)
  "Crawls a site starting from a given url. This version crawls
  each individual \"level\" concurrently. But in my experience it
  starts to fail if the concurrency grows unbounded."
  (if (stringp url-or-hash-table)
      (let ((normalized-url (url-recreate-url (url-normalize-url-for-sitecrawl (url-generic-parse-url url-or-hash-table)))))
	(fixed-point #'crawl-site-one-level-parallel (ht (normalized-url :crawl-me)) depth-limit))
    (fixed-point #'crawl-site-one-level-parallel url-or-hash-table depth-limit)))

(defun crawl-site-one-level-limited-parallel (site-hash-table)
  "Crawls one level of websites but tries to do in parallel while
limiting the amount of parallelism because in my previous
implementation where the parallelism was not limited, that
function failed sometimes presumably because emacs could not keep
up but I didn't really dig into it.

Originally I wanted to make use of the function
`url-queue-retrieve' since it limits the parallelism for you but
I don't think it's really viable here since everything happens
with timers so I can't easily get a handle on the asyncronous
process so I can call `accept-process-output' on it. I tried
calling `accept-process-output' with no arguments since it says
it will wait for any ol' process's output but it didn't seem to
work."
  (let* ((urls-to-crawl (ht-collect-keys (lambda (key val) (eq :crawl-me val)) site-hash-table))
	 (counter 0)
	 (max-parallel-processes 6)
	 (num-executing-processes 0)
	 (async-processes nil)
	 (cur-async-process 0)
	 (site-hash-table (ht-copy site-hash-table)))
    (while (< counter (length urls-to-crawl))
      (while (< num-executing-processes max-parallel-processes)
	(setq num-executing-processes (1+ num-executing-processes))
	(setq async-processes
	      (append async-processes
		      (list (url-retrieve (nth counter urls-to-crawl)
					  (lambda (url-to-crawl)
					    (let ((discovered-urls (-distinct (parse-buffer-as-html-and-get-urls-within-same-site (current-buffer) url-to-crawl))))
					      (ht-set! site-hash-table url-to-crawl discovered-urls)
					      (-each discovered-urls
						(lambda (discovered-url)
						  (unless (ht-contains? site-hash-table discovered-url)
						    (ht-set! site-hash-table discovered-url :crawl-me)))))
					    (setq counter (1+ counter))
					    (setq num-executing-processes (1- num-executing-processes))))))))
      (accept-process-output (get-buffer-process (nth cur-async-process async-processes)))
      (setq cur-async-process (1+ cur-async-process)))
    (message "counter %d" counter)
    (message (pp-to-string async-processes))
    site-hash-table))

(defun crawl-site5 (url-or-hash-table &optional depth-limit)
  "Crawls a site starting from a given url. This version crawls
  each individual \"level\" concurrently while limiting the
  concurrency."
  (if (stringp url-or-hash-table)
      (let ((normalized-url (url-recreate-url (url-normalize-url-for-sitecrawl (url-generic-parse-url url-or-hash-table)))))
	(fixed-point #'crawl-site-one-level-queued-parallel (ht (normalized-url :crawl-me)) depth-limit))
    (fixed-point #'crawl-site-one-level-queued-parallel url-or-hash-table depth-limit)))

(comment
 (benchmark-elapse (setq tmp4 (crawl-site4 "https://en.wikipedia.org/wiki/URI_normalization" 2)))
 (benchmark-elapse (setq tmp (crawl-site "https://en.wikipedia.org/wiki/URI_normalization" 2)))
 (benchmark-elapse (setq tmp (crawl-site "http://learnyouahaskell.com/")))
 (benchmark-elapse (setq tmp4 (crawl-site4 "http://learnyouahaskell.com/")))
 (ht->alist tmp4)
 (car (ht-collect-keys (lambda (k v) (eq :crawl-me v)) tmp))"http://learnyouahaskell.com/chapters/zippers/introduction/zippers"

 (url-retrieve "https://www.braveclojure.com/"
	       (lambda (&rest ignored)
		 (message "does this work??????")
		 (message (current-buffer))))
 (url-retrieve "http://example.com"
	       (lambda (&rest ignored)
		 (message "does this work??????")
		 (message (current-buffer))))

 (url-retrieve "http://example.com"
               (lambda (status start-time)
                 (message "The request is completed in %f seconds"
                          (float-time (time-subtract nil start-time)))
		 )
               `(,(current-time))
               'silent
               'inhibit-cookies))

;; TODO: Once we write a bunch of website crawling algorithms,
;; parallelize them to see what that looks like.

(defun reload-webserver (handlers port)
  "A utility function to reload a webserver. Useful for local
testing."
  (ws-stop-all)
  (ws-start handlers port))

(cl-defmacro with-webserver ((server handlers port) &rest body)
  "Starts a web server (binding it to the variable SERVER) with
HANDLERS and PORT, executes BODY, then stops the web server.
Original motivation for writing this was I wanted to unit test a
function which interacts with a web server and wanted to make
sure the web server got spun down after the test ran.

Note that `cl-defmacro' is used over `defmacro' for it's
destructuring capabilities. The structure of this macro is
similar to that of CL's with-open-file:
http://clhs.lisp.se/Body/m_w_open.htm"
  (declare (indent 1) (debug t))
  `(let (,server)
     (unwind-protect
	 (progn
	   (setq ,server (ws-start ,handlers ,port))
	   ,@body)
       (when ,server
	 (ws-stop ,server)))))

(defun network-process-local-port (network-server-process)
  "Returns the local port of a network process created by
`make-network-process'. Created because I wanted to spin up a web
server during a unit test and have the port be randomly assigned
which means I have to read the port back out during the test and
that logic was juuuust confusing enough to read that I wanted to
wrap it up in a nice human readable name.

Looks like I could also maybe use the
`network-lookup-address-info' function here. Probably should...
eh, it's fine."
  (array-last-item (process-contact network-server-process :local)))

(comment
 (reload-webserver '(((:GET . "^/specific/path$") .
		      (lambda (request)
			(with-slots (process headers) request
			  (ws-response-header process 200 '("Content-type" . "text/html"))
			  (process-send-string process (concat "inside of a GET ONE with path " (cdr (assoc :GET headers)))))))
		     ((:GET . "^/wow$") .
		      (lambda (request)
			(with-slots (process headers) request
			  (ws-response-header process 200 '("Content-type" . "text/html"))
			  (process-send-string process (concat "inside of a GET TWO with path " (cdr (assoc :GET headers))))
			  (process-send-string process (pp-to-string (array-last-item (process-contact process :local))))
			  (process-send-string process (pp-to-string (ws-port request)))
			  (comment  (process-send-string process (pp-to-string request))))))
		     ((:GET . "^/request-object$") .
		      (lambda (request)
			(with-slots (process headers) request
			  (ws-response-header process 200 '("Content-type" . "text/html"))
			  (process-send-string process (pp-to-string request)))))
		     ((:POST . ".*") .
		      (lambda (request)
			(with-slots (process headers) request
			  (ws-response-header process 200 '("Content-type" . "text/html"))
			  (process-send-string process "inside of a post!")))))
		   9000)

 (setf (ws-handlers lucas-test) nil)

 (with-webserver (server '(((:GET . "^/specific/path$") .
			    (lambda (request)
			      (with-slots (process headers) request
				(ws-response-header process 200 '("Content-type" . "text/html"))
				(process-send-string process (concat "inside of a GET ONE with path " (cdr (assoc :GET headers)))))))
			   ((:GET . "^/wow$") .
			    (lambda (request)
			      (with-slots (process headers) request
				(ws-response-header process 200 '("Content-type" . "text/html"))
				(process-send-string process (concat "inside of a GET TWO with path " (cdr (assoc :GET headers))))
				(process-send-string process (pp-to-string (array-last-item (process-contact process :local))))
				(process-send-string process (pp-to-string (ws-port request)))
				(comment  (process-send-string process (pp-to-string request))))))
			   ((:GET . "^/request-object$") .
			    (lambda (request)
			      (with-slots (process headers) request
				(ws-response-header process 200 '("Content-type" . "text/html"))
				(process-send-string process (pp-to-string request)))))
			   ((:POST . ".*") .
			    (lambda (request)
			      (with-slots (process headers) request
				(ws-response-header process 200 '("Content-type" . "text/html"))
				(process-send-string process "inside of a post!")))))
			 t)
		 (message (pp-to-string (network-process-local-port (slot-value server :process))))
		 (while t (sit-for 1)))

 )

;; (slot-value (car ws-servers) :port)

;; The built in testing framework for emacs is called ERT. There is a
;; lovely info manual about it so give that a read for more detail.
;; You can run ERT tests by running "M-x ert" and specifying which
;; test to run. If the test fails you can press 'b' to view the
;; backtrace or press 'd' to rerun the test with debugging enabled.

;; TODO: Defining a test in elisp is pretty straightforward actually,
;; the remaining work for me is twofold: ONE is that I should probably
;; setup a local webserver for testing instead of hitting an existing
;; one. I bet elisp could do this. TWO is that when comparing hash
;; tables, the ERT output is not at all clear about where the hash
;; tables differ. Turns out that ERT CAN better indicate where the
;; difference is but that has to be set up properly. For example, the
;; function "equal" has this property on it: (get 'equal
;; 'ert-explainer) which points to a function ert--explain-equal. I
;; would need to do something similar for ht-equals? (or my equals?
;; function perhaps) to get that to work properly. I'll read up on the
;; documentation to better understand that.

(ert-deftest crawl-site-test ()
  "Test out my function(s) which crawls a website. Motivation for
creating this unit test (besides learning about the unit testing
framework within emacs) was that I wanted to create lots of
different implementations for this website crawler (just to
experiment with different ideas and to explore what elisp can do)
but before doing that I wanted to get this test in place to
ensure that my different implementations were actually
equivalent.

An aside, I think there are two reasons I like writing tests:

1. It gives me confidence that I can refactor or try different
implementations of the code and things will still work.

2. It serves as up-to-date documentation for how to call the
function under test.

TODO: I would be interested in getting emacs to ask for any
arbitrary open port for these tests. Perhaps this could serve as
motivation:
https://github.com/eudoxia0/find-port/blob/master/src/find-port.lisp"
  ;; Defining a couple functions to generate the web server handlers
  ;; from the graph of links. I *think* you could say that the
  ;; webserver and the graph of links are "isomorphic" structures
  ;; because we have a function to transform the graph into the web
  ;; server (these functions) and another function (our crawl-site
  ;; function) to convert the webserver back into the links graph.
  ;; Might be wrong but it sure sounds fun to say:
  ;; https://en.wikipedia.org/wiki/Isomorphism
  (cl-flet* ((generate-html
	      (source-link outgoing-links)
	      (xmlgen `(html
			(body
			 (h1 (concat "Welcome to page: " ,source-link))
			 (a :href "https://google.com" "a link that should not be followed")
			 (p "welcome to the page!")
			 ,@(-map (lambda (outgoing-link)
				   `(div (a :href ,outgoing-link ,outgoing-link)
					 (hr)))
				 outgoing-links)
			 (div (p "bye now")
			      (a :href "https://www.gnu.org/software/emacs/manual/html_node/elisp/index.html" "another link that should not be followed"))))))
	     (generate-handlers
	      (links-graph)
	      (ht-map (lambda (source-link outgoing-links)
			(let ((source-path (car (url-path-and-query (url-generic-parse-url source-link)))))
			  `((:GET . ,(concat "^" source-path "$")) .
			    (lambda (request)
			      (with-slots (process headers) request
				(ws-response-header process 200 '("Content-type" . "text/html"))
				(process-send-string
				 process
				 ,(generate-html source-link outgoing-links)))))))
		      links-graph)))
    ;; Initially there are no handlers for the web server because we
    ;; have to figure out what port the server is on so we can
    ;; generate the graph of links so that we can generate the
    ;; handlers.
    (with-webserver (server nil t)
      (let* ((port (network-process-local-port (ws-process server)))
	     (links-graph (->> '(("/" . ("/one" "/two"))
				 ("/one" . ("/" "/four"))
				 ("/two" . ("/one" "/three"))
				 ("/three" . ("/"))
				 ("/four" . ()))
			       (-tree-map (-partial #'concat "http://localhost:" (number-to-string port)))
			       (ht<-alist)))
	     (url-to-crawl (concat "http://localhost:" (number-to-string port))))
	(setf (ws-handlers server) (generate-handlers links-graph))
	(should (equal? (crawl-site url-to-crawl) links-graph))
	(should (equal? (crawl-site2 url-to-crawl) links-graph))
	(should (equal? (crawl-site3 url-to-crawl) links-graph))
	(should (equal? (crawl-site4 url-to-crawl) links-graph))
	))))

;; TODO: I feel like I wish that every (?) emacs function invocation
;; was timed because when I've been playing around with this,
;; sometimes I'll run a function and after the fact I'll be curious
;; about how long it took to run so I'll have to re-invoke it with a
;; benchmark-elapse which seems silly.

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;;;; Messing around with implementing lazy lists.
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

;; TODO: Something which feels like an alternative (and already
;; existing) way to do "infinite" stuff in elisp is this:
;; https://www.gnu.org/software/emacs/manual/html_node/elisp/Generators.html
;; I'd be super interested in looking at that code too because I think
;; they do continuation stuff and I was reading about that online but
;; it would be cool to see a concrete implementation of it because I
;; don't understand how it works on an axiomatic level.
(defmacro lazy-cons (car cdr)
  "Creates a lazy cons cell (i.e. a cons cell wrapped inside a
thunk (i.e. an anonymous function with no args)) with CAR and
CDR. It's worth noting, that one should probably not mix laziness
with code that alters state outside of your program. Usually you
want a \"world\" altering operation to happen at a specific time
and introducing laziness reduces your control over when that side
effect happens. Doing things like modifying internal
state (perhaps a random number generator) or performing IO to
read data is probably fine though.

Related links:

- https://en.wikipedia.org/wiki/Thunk
- https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book-Z-H-24.html#%_sec_3.5
- https://www.classes.cs.uchicago.edu/archive/2017/spring/22300-1/lectures/LazyLists/index.html
- https://stuartsierra.com/2015/08/25/clojure-donts-lazy-effects
- http://blog.thezerobit.com/2012/07/28/lazy-sequences-in-common-lisp.html

An earlier implementation of this function was:

`(cons ,car (thunk-delay ,cdr))

Which I thought felt... I don't know, sort of elegant somehow.
It's actually how SICP implements their \"stream\" data structure
but part of me didn't like it because the very first element of
the list is NOT lazy hence why I scrapped it. Ultimately it
probably doesn't matter (this is just me playing around after
all) but I liked the idea of having a TRULY lazy list.

Before reaching the current implementation I also tried something
like this to achieve a truly lazy list:

`(cons (thunk-delay ,car) (thunk-delay ,cdr))

But I didn't like that since it introduces so many extra thunks
that need to be evaluated."
  `(thunk-delay (cons ,car ,cdr)))

(defun lazy-car (l)
  "Gets the CAR of a lazy list L. Returns nil if L is nil just
like the car function."
  (if (null l)
      nil
    (car (thunk-force l))))

(defun lazy-cdr (l)
  "Gets the CDR of lazy list L. Returns nil if L is nil just like
the cdr function."
  (if (null l)
      nil
    (cdr (thunk-force l))))

(defun lazy-iterate (f x)
  "Returns a lazy sequence of X, (F X), (F (F X)), etc... In
mathematical terms I believe you would say it returns the
\"orbit\" of X. Implementing this was inspired by the function
\"iterate\" in clojure.

Related links:

- https://en.wikipedia.org/wiki/Iterated_function
- https://en.wikipedia.org/wiki/Orbit_(dynamics)
- https://clojuredocs.org/clojure.core/iterate"
  (lazy-cons x (lazy-iterate f (funcall f x))))

;; TODO: We have a lot of recursion with these functions. I like
;; writing things like this but a lot of them aren't tail recursive so
;; they'd probably be slow. Maybe I should write a lower level
;; function like "reduce" and have it be more iterative then the rest
;; of these functions can build off that. They'll still be nice but
;; also be more efficient. Or, heck, maybe I just leave it alone!
(defun-multi-arity lazy-range
  "Basically the same implementation as clojure's \"range\"
function. Other languages have a similarly named function too.
I'm not actually sure if I like this function name much. I say
that because the word \"range\" in mathematics already has a
meaning relating to the values that a function produces. I've
also seen the word as describing the difference between a low and
high value. In the SICP book, they have a function named
\"enumerate-interval\" which I think is more explicit and might
like better.

I'm honestly not sure if I even like this function implementation
because I'm not sure I like it when the same argument position in
a function has a different meaning depending on how many
arguments follow. I think I prefer a little more... consistency?
explicitness? I mean, I guess doing stuff like this effectively
means you get more power out of the same word, but it also opens
the door to a bit of confusion. I think it's the kind of thing
where, for veterans, it's probably powerful but, for newcomers,
it might be a bit confusing at first.

Related links:
- https://clojuredocs.org/clojure.core/range
- https://docs.python.org/3/library/stdtypes.html?highlight=range#range
- https://www.php.net/manual/en/function.range.php
- https://www.mathsisfun.com/definitions/range-statistics-.html
- https://en.wikipedia.org/wiki/Range_of_a_function
- https://www.gnu.org/software/emacs/manual/html_node/elisp/Float-Basics.html"
  (() (lazy-range 1.0e+INF))
  ((end) (lazy-range 0 end))
  ((start end) (lazy-range start end 1))
  ((start end step)
   (if (>= start end)
       nil
     (lazy-cons start (lazy-range (+ start step) end step)))))

(defun lazy-reduce-from (reducing-fn init l)
  "The lazy equivalent of the dash feature's -reduce-from
function."
  (let ((acc init))
    (while l
      (setq acc (funcall reducing-fn acc (lazy-car l)))
      (setq l (lazy-cdr l)))
    acc))

(defun lazy-reduce (reducing-fn l)
  "The lazy equivalent of the dash feature's -reduce function."
  (if l
      (lazy-reduce-from reducing-fn (lazy-car l) (lazy-cdr l))
    (reducing-fn)))

(defun lazy-reduce-r-from (reducing-fn init l)
  "The lazy equivalent of the dash feature's -reduce-r-from
function which does the reduction on the list in reverse. Note
that the reducing function signature has the accumulator
parameter as the second argument instead of the first like the
other reducing function. Strictly speaking, it doesn't have to be
that way but it's done that way in haskell too with it's
\"foldr\" function so I assume folks thought it made more sense.
I suppose maybe they did it because you can envision the
accumulated value coming down from the right side of the list,
heading towards the original invocation of the reducing
function."
  (if l
      (funcall reducing-fn
	       (lazy-car l)
	       (lazy-reduce-r-from reducing-fn init (lazy-cdr l)))
    init))

(defun-multi-arity lazy-reduce-clojure
  "The lazy equivalent of a \"reduce\" function. Operates like
clojure's reduce implementation minus the ability of the reducing
function to call the function \"reduced\" to terminate the
computation early: https://clojuredocs.org/clojure.core/reduce

I remember when I initially saw clojure's reduce function (and
maybe I saw something similar elsewhere?) I thought (as I have
before with multiple arity functions) that the parameters were
confusing. I think it was the fact that the function was new to
me plus the fact that it could take up to 3 parameters and I kept
forgetting which ones went where and it didn't help that the
second parameter was different depending on if a third parameter
was passed. I also thought that the whole business of the
reducing function calling itself with no arguments when the list
is empty just felt weird. Part of me likes it a little more now
that I'm used to it but part of me also wants to keep it simpler.
Still, I just wanted to define this because it feels so cool to
use my home grown `defun-multi-arity' macro."
  ((reducing-fn l)
   (if l
       (lazy-reduce reducing-fn (lazy-car l) (lazy-cdr l))
     (reducing-fn)))
  ((reducing-fn init l)
   (let ((acc init))
     (while l
       (setq acc (funcall reducing-fn acc (lazy-car l)))
       (setq l (lazy-cdr l)))
     acc)))

;; TODO: This was an initial implementation of the lazy-reduce-from
;; function but it DOES NOT WORK even though I feel like it should.
;; The reason it doesn't work is because it seems that you cannot bind
;; a symbol XYZ to the value of another symbol XYZ in the lexical
;; closure. I feel like it should work though since you can do
;; something like (let ((x 1)) (let ((x (1+ x))) x)). I almost feel
;; like that should be changed. I wonder if I'm right or there's
;; something I'm missing.
(comment
 (defun lazy-reduce-from (reducing-fn init l)
   "The lazy equivalent of the dash feature's -reduce-from
function."
   (cl-loop for acc = init then (funcall reducing-fn acc (lazy-car ll))
	    for l = l then (lazy-cdr l)
	    while l
	    finally return acc)))

(defun lazy-nth (n l)
  "Get the Nth element from lazy list L. I would love to write it
in a tail recursive manner (since I always feel like those
implementations are clean), but unfortunately elisp doesn't do
tail call optimization so, for large lists, it quickly exceeds
the `max-lisp-eval-depth' variable and we'd error out."
  (cl-loop for ll = l then (lazy-cdr ll)
	   while ll
	   repeat n
	   finally return (lazy-car ll)))

(defun lazy-list-helper-fn (l)
  (if (null l)
      'nil
    `(lazy-cons ,(car l) ,(lazy-list-helper-fn (cdr l)))))

(defmacro lazy-list (&rest objects)
  (lazy-list-helper-fn objects))

(defun lazy-take (n l)
  (if (or (<= n 0)
	   (null l))
      nil
    (lazy-cons (lazy-car l) (lazy-take (1- n) (lazy-cdr l)))))

(defun lazy-list-to-list (l)
  "Converts a lazy list to a list. Note that this current
implementation does not do this conversion recursively (so any
nested lazy lists will still be lazy)."
  (cl-loop for ll = l then (lazy-cdr ll)
	   while ll
	   collect (lazy-car ll)))

(defun lazy-map (f l)
  (if l
      (lazy-cons (funcall f (lazy-car l))
		 (lazy-map f (lazy-cdr l)))
    nil))

(defun lazy-map-with-reduce (f l)
  "Implementing the map function with a reduce just for fun."
  (lazy-reduce-r-from (lambda (x acc) (lazy-cons (funcall f x) acc)) nil l))

(defun lazy-filter (pred l)
  "Filter list L, keeping only values for which PRED returns
true."
  (if l
      (if (funcall pred (lazy-car l))
	  (lazy-cons (lazy-car l) (lazy-filter pred (lazy-cdr l)))
	(lazy-filter pred (lazy-cdr l)))
    nil))

(defun lazy-filter-with-reduce (pred l)
  "Implementing the filter functionality with a reduce just for
fun."
  (lazy-reduce-r-from (lambda (x acc)
			(if (funcall pred x)
			    (lazy-cons x acc)
			  acc))
		      nil
		      l))

(defun complement (f)
  "Returns a function which returns the opposite truth value of
f. Inspired by https://clojuredocs.org/clojure.core/complement.
Turns out this function already existed in the dash library under
the name \"-not\" so I started using that instead. Still kept
this around for fun though.

This function is known as a combinator which is basically a
function that returns another function:
https://en.wikipedia.org/wiki/Combinatory_logic"
  (lambda (&rest args)
    (not (apply f args))))

(defun lazy-remove (pred l)
  "Opposite of `lazy-filter'."
  (lazy-filter (-not pred) l))

(defun lazy-append (&rest lists)
  "The lazy version of the `append' function.

I had tough time writing this function. Maybe I was just tired
but it took me a minute to even grok how to write a basic
`append' function. I feel like this is a recurring thing with me
where building stuff up in a functional manner (in this case via
cons cells) feels so restrictive sometimes and I know there's a
certain method of iteration that will solve the problem neatly
but I don't see it right away. It feels like I'm trapped somehow.
It's like I'm on one floor of a building with a value and I see
the rest of the value I need and I want to just grab it and slap
it onto my value and be done but I can't because that second
value is on a different floor than mine. Anywho.

It also took me a second to realize that ARGUMENTS is a regular
ol' list of lazy lists. It kind of confused me for a second to
think about mixing lazy and non-lazy lists."
  (cl-labels ((lazy-append-helper
	       (l1 acc)
	       (if l1
		   (lazy-cons (lazy-car l1) (lazy-append-helper (lazy-cdr l1) acc))
		 acc)))
    (-reduce-r-from #'lazy-append-helper nil lists)))

(defun lazy-apply (f &rest arguments)
  "Like `apply' but works when the last argument in ARGUMENTS is
a lazy list. Note that the lazy list gets realized when this
happens but I think there's no way around that."
  (apply f (append (-butlast arguments)
		   (lazy-list-to-list (-last-item arguments)))))

(defun lazy-mapcat (f l)
  "The lazy version of the `mapcat' function. It's important to
note here that F must return lazy lists for this to work. So the
value that `lazy-append' is getting `lazy-apply'd to is really a
lazy list of lazy lists. What a wild world we live in. An
alternative implementation for this function is:

  (lazy-reduce-from #'lazy-append nil (lazy-map f l))
"
   (lazy-apply #'lazy-append (lazy-map f l)))

(defun lazy-cartesian-product (xs ys &rest more)
  "The lazy version of the cartesian product algorithm:
https://en.wikipedia.org/wiki/Cartesian_product"
  (if more
      (lazy-mapcat (lambda (x)
		     (lazy-map (lambda (z) (cons x z))
			       (apply #'lazy-cartesian-product ys (car more) (cdr more))))
		   xs)
    (lazy-mapcat (lambda (x)
		   (lazy-map (lambda (y)
			       (list x y))
			     ys))
		 xs)))

;; TODO: How to properly handle redirects? Like, if we do
;; https://braveclojure.com then I think it redirects to
;; https://www.braveclojure.com but I wouldn't crawl that because now
;; it's at a different host so my code thinks it's a different site. I
;; guess we need to keep a set of urls hosts which are considered the
;; same site?

;; TODO: Is there a way to inspect the progress of the crawling as it
;; goes? I think that would be cool because then I wouldn't need to
;; have things like print statements.

;; TODO: How do I have a process like this running in the background
;; so I can still do other stuff? If I do that I'd like to see how to
;; inspect said process too!

;; TODO: What happens if a url does not actually exist or something
;; like that? General error handling I guess.

;; TODO: Create a function to download all the pages of a particular
;; website and rewrite any links so that they point to local files.
;; Use case is that sometimes I've read ebook-like things online and
;; sometimes I've wanted to be able to do that when going on an
;; airplane or somewhere else without internet

;; TODO: A variation on the fixed point idea. Define the function
;; which gets repeatedly applied but have it also accept a number
;; representing the amount of work to do. During it's work the
;; function would reduce this number and pass it back to a new
;; modified fixed-point function. The motivation here is that I didn't
;; like how the fixed-point function I originally defined can only put
;; a limit on the number of times f is called but (especially for
;; website crawling) one invocation of f could take a VERY different
;; amount of time than another invocation depending on how many
;; websites we had to crawl. So, by having this number we could
;; control it in a more fine grained way which feels interesting.

;; TODO: In the file backquote.el the ` macro is defined. Originally I
;; thought it was a reader macro but it turns out to be just a
;; defalias: (defalias '\` (symbol-function 'backquote)). I wonder if
;; I can define other things like that. I'm kind of fascinated
;; honestly, using that backtick doesn't feel like calling a normal
;; function and yet it's possible apparently.

(defmacro if-let-alist (alist then &rest else)
  "I thought the `let-alist' macro was neat and had a situation
where I needed to check if an alist was non-nil before grabbing
values from it. So, I made this macro to make my code a biiiiit
more concise (but also because writing macros is fun).

An almost equivalent alternative to using something like this is
to use dash library functions like `-when-let':

(-when-let ((&alist 'one 'two 'three) '((one . 1) (two . 2) (three . 3)))
  (+ one two three))

The main difference (besides of course having to explicitly
declare which values from the alist you want to use) is that if a
key doesn't exist, then the overall condition is deemed false:

(-when-let ((&alist 'does-not-exist) '((hello . 1)))
  'will-not-be-reached)

I'm not sure if I like that or not. Part of me wonders if it
would be nice to decouple the value being non-nil from the
destructuring of said value. I think clojure behaves like that.
Just a thought!"
  (declare (indent 2))
  `(if ,alist
       (let-alist ,alist
	 ,then)
     ,@else))

(defmacro when-let-alist (alist &rest body)
  "These condition+binding things often seem to have an `if' and
`when' variation so here is the `when' one! Funnily there doesn't
seem to be an `unless' variation that I can see (I've checked in
the dash library, clojure, and common lisp), I wonder why that
is. Just not as  "
  (declare (indent 1))
  `(if-let-alist ,alist (progn ,@body)))

(defun case-insensitive-string= (s1 s2)
  "Does a case insensitive comparison of strings."
  (string= (downcase s1) (downcase s2)))

(defun couch-surfing-cost (start-date end-date person-staying-with)
  "A function to help calculate how much money I should be paying
my friends/family when I stay with them starting on START-DATE
and leave on END-DATE. I'll pay with based on the number of
nights so END-DATE is kind of exclusive rather than inclusive.
It's basically your typical hotel staying experience.

Motivation is that when I move to NYC on 2021-11-22, I'm gonna
stay with friends and family for a while before finding my own
place. I want to, of course, pay them for this and I wanted the
payments to be accurate!

Rough description of how this algorithm works: If I stay with 2
people then I'll divide the rent by 3 (to find out how much I'd
pay if we split it equally), then I'll divide that by the number
of days in the month (so I can get an accurate payment of how
much I should pay per day, note that the days in the month might
be different depending on the dates I'm staying with someone),
then I'll multiply that by a something like 1.25 since I want to
pay a little extra to thank them for hosting me."
  (if-let* ((places-to-stay '(((people . ("Emily" "Evan"))
			       (total-rent . 2400.0)
			       (multiplier . 1.0))
			      ((people . ("Jane" "Tori"))
			       (total-rent . 2300.0)
			       (multiplier . 1.25))
			      ((people . ("Joe"))
			       (total-rent . 1600.0)
			       (multiplier . 1.25))
			      ((people . ("Mom" "Dad"))
			       (total-rent . 0.0)
			       (multiplier . 0.0))))
	    (staying-at-place (let ((-compare-fn #'case-insensitive-string=))
				(-find (lambda (place-to-stay)
					 (-contains? (cdr (assoc 'people place-to-stay)) person-staying-with))
				       places-to-stay)))
	    (start-date (iso8601-parse start-date))
	    (end-date (iso8601-parse end-date)))
      (->> start-date
	   ;; Unfold is pretty neat! I think this is the first time
	   ;; I've seen something like it. I suppose it's similar in
	   ;; function to a function like "iterate" but more generic
	   ;; in that you can terminate the list.
	   (-unfold (lambda (start-date)
		      (if (and (= (decoded-time-month start-date)
				  (decoded-time-month end-date))
			       (= (decoded-time-day start-date)
				  (decoded-time-day end-date))
			       (= (decoded-time-year start-date)
				  (decoded-time-year end-date)))
			  nil
			;; For some reason doing this decoded-time-add
			;; call generates the message "obsolete
			;; timestamp with cdr 1 [70 times]". This all
			;; works though so I don't really care enough.
			(cons start-date (decoded-time-add start-date (make-decoded-time :day 1))))))
	   (-map (lambda (dt) (date-days-in-month (decoded-time-year dt) (decoded-time-month dt))))
	   (-map (lambda (num-days-in-month)
		   (let-alist staying-at-place
		     (* .multiplier
			(/ .total-rent
			   (1+ (length .people))
			   num-days-in-month)))))
	   (-sum))))

(defun cartesian-product (xs ys &rest more)
  "Calculates the cartesian product of two or more lists:
https://en.wikipedia.org/wiki/Cartesian_product

I feel like this implementation confuses me. The nesting \"map\"
functions just feels hard for me to grok. I get it but it's still
confusing. It also feels weird to me that we're transforming an
element X into a list of all pairs with X and then concatting
that down with all neighboring lists. It works but it feels so
much more natural in my mind to just keep appending the next pair
to an existing list. Maybe that's just my procedural brain
talking but I wonder if I could simplify this at all and what
that would look like. Maybe just a helper function? Maybe take a
more iterative approach? Or heck, maybe this implementation is
fine and my brain is just not used to parsing nested map
functions. This feels like one of those moments where my brain
fights with functional programming'esq stuff. In my head, I want
to do a PRECISE addition of one element at a time in the heart of
the nested loop but here I kind of have to build up a big value
and then concat it with it's neighbors. Maybe that is just the
functional way but I guess it feels expensive in my mind but
honestly I don't even know how expensive something like that is.
TODO: See if I can get a cartesian product implementation I'm
happy with."
  (if more
      (-mapcat (lambda (x)
		 (-map (lambda (z) (cons x z))
		       (apply #'cartesian-product ys (car more) (cdr more))))
	       xs)
    (-mapcat (lambda (x)
	       (-map (lambda (y) (list x y)) ys))
	     xs)))

(defun cartesian-product2 (&rest lists)
  "Here is a second implementation of the cartesian product
algorithm which I think I like better. I think I like it better
because it consistently does a `cons' when building the result as
opposed to the previous implementation which does a `list' to get
a base tuple and then `cons'es onto that when more than two sets
are passed. I also like that I don't have those nested lambdas
anymore. I still feel like those are confusing to read. I could
probably make little helper functions like this in the previous
code too but it feels... wrong? Because the solution is recursive
so it'll be redefining those functions at every level... I don't
know, it just feels like extra work.

This implementation still does the mapcat thing though, I'm still
not sure how to get rid of it.

This function also behaves a little differently than it's
previous implementation because it allows 1 or 0 arguments to be
passed but I'm not sure what situations we'd want to pass fewer
than 2 arguments anyway."
  (cl-flet* ((cons-onto-each-elem
	      (x ys)
	      (-map (-partial #'cons x) ys))
	     (reducer
	      (xs acc)
	      (-mapcat (lambda (x) (cons-onto-each-elem x acc)) xs)))
    (-reduce-r-from #'reducer '(()) lists)))

(defun cartesian-product3 (&rest lists)
  "Another way to do cartesian product. The order of elements is
backwards compared to the previous implementation but I feel like
it shouldn't really matter if you're treating this like a set.
You could also reverse the LISTS argument and you'd get the same
order.

Trying to write this one without looking at my previous solution
made me feel that old familiar feeling that I get when I program
in a functional manner, it just feels hard. Sometimes it feels
like I can only do it perfectly or not at all. When I get the
solution it feels great but getting there felt tough for some
reason. Feels like it takes more mental gymnastics or something."
  (cl-flet* ((cons-onto-each-elem
	      (x ys)
	      (-map (-partial #'cons x) ys))
	     (reducer
	      (acc xs)
	      (-mapcat (lambda (x) (cons-onto-each-elem x acc)) xs)))
    (-reduce-from #'reducer
		  '(())
		  lists)))

(defun crossword-brute-enumerate-words ()
  "Sometimes you get to a point in a crossword where you have an
unfinished word with only a couple blank spots and you can't come
up with the answer but you suspect that you could recognize the
answer if it was shown to you. I wrote this function to help with
that.

TODO: I just did it for this one word. How do we generalize this
though? We could generalize it for the number of blank spots AND
have it work for any word with letters in differing locations.
Maybe the input could be something like:

\"ga__ry\"

and it tries to fill in the blank spots. Or it would be cool if
we could get even more granualar and specify a set of letters we
want to fill in a certain spot (because we typically can narrow
down which letters just \"don't fit\" based on our english
knowledge).

Also it would be cool to lookup these words in the dictionary to
filter out any non-words. That feels a little like cheating but
maybe it could be a last resort sort of thing if it's really
tough figuring out what the word is."
  (let ((acc nil)
	(alphabet "abcdefghijklmnopqrstuvwxyz"))
    (cl-loop for i across alphabet
	     do (cl-loop for j across alphabet
			 do (setq acc (cons (concat "ga" (char-to-string i) (char-to-string j) "ry") acc))))
    acc))

;; TODO: It known far and wide that I like coming up with different
;; implementations for functions. Normally I'll just write them next
;; to eachother and append a number to the function name and maybe
;; eventually get rid of the implementations I like less, leaving me
;; with just one. But what if I made alternate implementations of a
;; function a little construct? Like what if I made a macro and I was
;; able to define multiple definitions of the function? The first one
;; would be stored in the function slot of the symbol as normal (at
;; least I think that's how this happens?) and the rest of them could
;; get bound to other slots on the symbol? You could then even have it
;; so unit testing that function would actually test each
;; implementation to ensure they are equivalent! You could do
;; benchmarks on them to! Hmmmm sounds fun.

;; TODO: I think I'd like to implement Paul Rutledge's graph related
;; code:
;; https://github.com/vodori/missing/blob/develop/src/missing/topology.clj
;; Furthermore, I think it would be fun to write those algorithms to
;; work for both hash tables and alists. In doing so I like the idea
;; of writing some generic axiomatic graph functions from which the
;; more complicated functions can be achieved.

(defun elisp-intro-exercise-narrowing ()
  "Displays the first 60 characters of the current buffer, even
  if the buffer is narrowed somewhere else. An exercise that the
  \"intro to elisp\" book proposes."
  (interactive)
  (save-restriction
    (widen)
    (message (buffer-substring-no-properties (point-min) 61))))

;; For the last couple days (today is 2021-12-05) I've been working on
;; creating a parallel implementation of the website crawler. I got AN
;; implementation working but it seems buggy in that I've seen it not
;; work when trying to crawl >50 links at the same time. It seems like
;; perhaps I'm bumping up against some sort of limit and it causes the
;; http requests to fail and also causes emacs to lock up more than
;; usual. That is obviously not good but I also just don't feel
;; confident in my knowledge of how my implementation works (it uses a
;; function accept-process-output which seems to get things working
;; but I don't fully understand everything that's going on). Since I
;; don't feel like I understand concurrency/parallelism primitives
;; well in emacs I'm taking a step back to try and grok that. So let's begin.

;; First off, Emacs is SINGLE THREADED, so there is no true
;; parallelism within emacs. In other words, if elisp code is
;; executing, that is the ONLY elisp code that is executing. Some
;; related links: https://tkf.github.io/2013/06/04/Emacs-is-dead.html,
;; https://www.emacswiki.org/emacs/NoThreading,
;; https://www.emacswiki.org/emacs/ConcurrentEmacs,
;; https://nullprogram.com/blog/2018/05/31/ Although you'll probably
;; never be as computationally efficient as something that can be
;; truly multi-threaded, I think you can still get pretty far in terms
;; of executing things at the "same time". There are "timers" which
;; let you execute code after a configured amount of time has passed.
;; A good generalized example of this is
;; https://www.emacswiki.org/emacs/LaterDo. Note that if your code is
;; being run by a timer, it is still the ONLY code that runs (since,
;; again, emacs is single threaded) so if it hangs then emacs itself
;; will hang. Part of me wonders if, under the hood in the C
;; implementation, there might be some multi-threading going on so
;; that input is constantly read (because you can type C-g to quit out
;; of the current computation whenever) and timers execute accurately
;; but then on the level of elisp it's always single threaded. Anyway.
;; So there are timers. There are also "processes" where emacs can
;; fork an external process. This process truly does run at the same
;; time as emacs (which makes sense since it's a totally different
;; process) but remember that any interaction with this process where
;; you're using elisp to send/receive data with the process will be
;; the ONLY THING that is happening in emacs land. For example, when
;; spawning a process you can specify a "filter" (i.e. an elisp
;; function) which will get invoked when the process outputs to stdout
;; but that filter function will only get called at certain times in
;; emacs land like when emacs is waiting for input. So, if you were to
;; spawn a process then your code instantly goes into an infinite
;; loop, then even if the external process is sending back data, your
;; filter function will NEVER get called because that infinite loop is
;; preventing it. Emacs recently introduced something they call
;; "threads" which is another concurrency primitive where a thread can
;; do it's thing and then "yield" control to another thread. All in
;; all it feels like emacs does a lot of "cooperative" kinds of
;; concurrency where code is very aware that if it's running, then
;; other things are not and so it tries not to take up too much time
;; and stuff like that.

(defun mod-non-zero-based (base-num num modulo)
  "Performs a modulus operation but with the range of numbers
starting at an arbitrary base number BASE-NUM instead of 0 like
it normally is. Written for the `caesar-cipher' algorithm because
I had to repeat this computation and didn't like how confusing it
looked."
  (+ base-num (mod (- num base-num) (- modulo base-num))))

(defun caesar-cipher (s shift)
  "Created because cryptography is fun! Actually the original
motivation for writing this was because on 2021-12-07 on my old
macbook from college I found a file called \"ara\" (my first
girlfriend's name) and it was a file that just contained the
text:

znxr vbh pneq 5000 cbvagf + svir qevaxf nyfb qenj fnvgnzn ba vg
tvivat gur guhzof hc.

and I wanted to know what it was. It felt like a caesar cipher so
I wrote this function and then this snippet to generate all
possible combinations:

(dolist (s (cl-loop for shift from 0 to 25 collect (caesar-cipher \"znxr vbh pneq 5000 cbvagf + svir qevaxf nyfb qenj fnvgnzn ba vg tvivat gur guhzof hc.\" shift)))
  (insert s)
  (newline))

and it turns out my hunch was right! The correct sentence was:

make iou card 5000 points + five drinks also draw saitama on it
giving the thumbs up.

haha, I don't really remember the context around that but it
warms my heart to see it. I hope she's doing well."
  ;; TODO: Convert this to a threading macro but first write an elisp
  ;; command that will do the conversion for us.
  (concat
   (-map (lambda (c)
	   (cond
	    ((and (>= c ?a)
		  (<= c ?z))
	     (mod-non-zero-based ?a (+ c shift) (1+ ?z)))
	    ((and (>= c ?A)
		  (<= c ?Z))
	     (mod-non-zero-based ?A (+ c shift) (1+ ?Z)))
	    (t c)))
	 (append s nil))))

;; TODO: I remember seeing a book in barnes and noble a while back
;; about algorithms to generate mazes. I think I'd like to mess around
;; with generating some mazes in emacs. I could even make it a minor
;; mode (or whatever) where like C-n and C-p would not move the cursor
;; if they're hitting a wall.

(defun eval-infix-math-exp (exprs)
  (let (operand-stack
	operation-stack
	(op-precedence '((+ . 0)
			 (- . 0)
			 (* . 1)
			 (/ . 1))))
    (push (pop exprs) operand-stack)
    (push (pop exprs) operation-stack)
    (while exprs
      (if (listp (-first-item exprs))
	  (push (eval-infix-math-exp (pop exprs)) operand-stack)
	(push (pop exprs) operand-stack))
      (when (or (null exprs)
		(>= (alist-get (-first-item operation-stack) op-precedence)
		    (alist-get (-first-item exprs) op-precedence)))
	(let ((right (pop operand-stack))
	      (op (pop operation-stack))
	      (left (pop operand-stack)))
	  (push (funcall op left right) operand-stack)))
      (when exprs
	(push (pop exprs) operation-stack)))
    (while operation-stack
      (let ((right (pop operand-stack))
	    (op (pop operation-stack))
	    (left (pop operand-stack)))
	(push (funcall op left right) operand-stack)))
    (pop operand-stack)))

(defmacro infix-math (&rest exprs)
  "A macro which allows you to write infix math expressions (i.e.
what we're used to doing) and converts them to the appropriate
lisp code. Just wrote it for fun."
  `(eval-infix-math-exp ',exprs))

;; (infix-math 1 + 2) -> (+ 1 2)
;; (infix-math 1 + 2 * 3) -> (+ 1 (* 2 3))
;; (infix-math 1 + 2 * 3 / 4 - 1) -> (- (+ 1 (/ (* 2 3) 4)) 1)
;; (infix-math 1 + 2 * (3 - 4)) -> (+ 1 (* 2 (- 3 4)))
